## 03\. 서비스 메시를 활용한 고급 트래픽 관리와 장애 주입(Fault Injection) 테스트

이제 우리의 클러스터 내부 통신은 mTLS라는 철벽 갑옷으로 완벽하게 보호된다. 하지만 서비스 메시의 진정한 힘은, 이 안전한 통로 위를 흐르는 트래픽의 '물결' 자체를 자유자재로 지휘하는 지휘자의 능력에서 발현된다. 9장에서 다룬 Ingress가 클러스터의 '대문'에서 호스트와 경로를 보고 문을 열어주는 문지기였다면, 서비스 메시는 도시 안의 모든 교차로에서 교통경찰이 되어, 특정 차량(요청)을 선별하고, 우회시키며, 때로는 의도적으로 정체시키는 정교한 교통 통제 시스템과 같다. 🚦

-----

### 고급 트래픽 관리: 가중치와 헤더를 이용한 정밀 제어

5장에서 우리는 디플로이먼트의 복제본 수를 조절하여 카나리 배포를 흉내 내는 원시적인 방법을 보았다. 서비스 메시는 이 과정을 예술의 경지로 끌어올린다. 사이드카 프록시는 L7 수준에서 트래픽을 이해하므로, 파드의 개수와는 무관하게 실제 요청의 \*\*가중치(weight)\*\*를 기준으로 트래픽을 정밀하게 분배할 수 있다.

Istio의 `VirtualService` 리소스를 통해, 우리는 다음과 같이 선언할 수 있다.

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - route:
    - destination:
        host: reviews
        subset: v1
      weight: 90 # 1. 트래픽의 90%는 v1으로 보낸다.
    - destination:
        host: reviews
        subset: v2
      weight: 10 # 2. 나머지 10%는 카나리 버전인 v2로 보낸다.
```

이제 우리는 파드의 개수를 신경 쓸 필요 없이, 오직 `weight` 값만을 변경하여 10% -\> 25% -\> 50% -\> 100%로 점진적인 릴리즈를 완벽하게 제어할 수 있다.

더 나아가, 우리는 단순히 가중치뿐만 아니라 HTTP 요청의 \*\*헤더(header)\*\*와 같은 내용을 기반으로 트래픽을 라우팅할 수 있다. 예를 들어, 우리 회사의 내부 테스터들에게만 새로운 기능을 먼저 공개하고 싶다면 다음과 같이 선언하면 된다.

```yaml
...
  http:
  - match:
    - headers: # 1. 만약 HTTP 헤더에...
        end-user:
          exact: "internal-tester" # ... 'end-user'가 'internal-tester'인 요청이라면,
    route:
    - destination:
        host: reviews
        subset: v2 # ... 무조건 v2로 보낸다.
  - route: # 2. 그 외의 모든 일반 사용자 요청은...
    - destination:
        host: reviews
        subset: v1 # ... 기존의 v1으로 보낸다.
```

이러한 정교한 트래픽 제어 능력은 A/B 테스팅, 다크 런칭(dark launching), 기능 플래그(feature flagging)와 같은 모든 현대적인 릴리즈 전략의 기술적 기반이 된다.

-----

### 장애 주입(Fault Injection): 평시에 흘리는 땀, 전시의 피를 막는다

최고의 군대는 평시에 실전과 같은 훈련을 통해 약점을 발견하고 보완한다. 마이크로서비스의 세계에서 이 훈련에 해당하는 것이 바로 \*\*장애 주입(Fault Injection)\*\*이다. 우리는 `payment-service`가 갑자기 5초간 응답이 없거나, 10%의 요청에 대해 HTTP 503 에러를 반환하는 상황을 의도적으로 시뮬레이션하여, `order-service`가 이러한 장애 상황을 얼마나 우아하게 처리하는지(타임아웃, 재시도, 서킷 브레이커 등)를 테스트할 수 있다.

과거에는 이를 위해 `payment-service`의 코드를 직접 수정하거나 서버를 실제로 내리는 위험한 방법을 사용해야 했다. 하지만 서비스 메시는 사이드카 프록시를 통해, 실제 `payment-service`는 전혀 건드리지 않고도 이러한 장애를 완벽하게 모방할 수 있다.

```yaml
...
  http:
  - route:
    - destination:
        host: ratings
        subset: v1
    fault: # 1. ratings v1으로 가는 모든 요청에 대해...
      delay:
        percentage:
          value: 10.0 # ... 10%의 확률로
        fixedDelay: 5s # ... 5초의 지연을 주입하고,
      abort:
        percentage:
          value: 5.0 # ... 5%의 확률로
        httpStatus: 503 # ... HTTP 503 에러를 반환하라.
```

이 정책을 적용하는 순간, 우리는 `ratings` 서비스 자체는 멀쩡히 살아있는 상태에서, 이 서비스와 통신하는 모든 클라이언트의 회복탄력성 메커니즘이 실제로 동작하는지를 안전하게 검증할 수 있다. 이것이 바로 \*\*카오스 엔지니어링(Chaos Engineering)\*\*의 정수다.

서비스 메시는 이처럼 네트워크를 단순한 데이터 전송 파이프라인에서, 우리가 원하는 대로 주무르고, 제어하며, 시험할 수 있는 지능적인 \*\*프로그래머블 프록시 계층(Programmable Proxy Layer)\*\*으로 바꾸어 놓는다.

이제 우리의 플랫폼은 애플리케이션의 배포와 운영뿐만 아니라, 그들 사이의 가장 복잡한 상호작용까지도 완벽하게 제어할 수 있는 경지에 이르렀다. 하지만 이 모든 강력한 기능들을 여러 팀과 부서가 함께 사용하는 **멀티테넌트(multi-tenant)** 환경에서, 우리는 어떻게 자원을 공평하게 나누고, 그 비용을 추적하며, 플랫폼 전체의 지속 가능성을 확보할 수 있을까? 다음 장에서는 이 현실적인 운영의 문제, **멀티테넌시와 비용 최적화**의 세계를 탐험할 것이다.