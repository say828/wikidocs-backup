## 03\. 자원 요청(Requests)과 제한(Limits): 클러스터 안정성을 위한 최소한의 약속

당신의 애플리케이션은 이제 스스로의 건강 상태를 보고할 줄 아는, 잘 만들어진 컨테이너 안에 담겨 있다. 하지만 이제 우리는 이 애플리케이션이 혼자 사는 세상이 아니라는 냉혹한 현실을 직시해야 한다. 쿠버네티스 클러스터는 CPU와 메모리라는 유한한 공공 자원을 수많은 애플리케이션(파드)들이 나누어 쓰는 일종의 **공동 주택**과도 같다.

만약 이 공동 주택에 아무런 규칙이 없다면 어떻게 될까? 어느 날 한 입주민(파드)이 갑자기 엄청난 전력(CPU)을 끌어다 쓰거나 수도(메모리)를 틀어놓고 잠가버린다면, 아파트 전체가 정전되거나 단수되는 사태가 벌어질 것이다. 이른바 **'시끄러운 이웃(Noisy Neighbor)'** 문제다. 하나의 잘못된 파드가 자신이 실행 중인 노드(Node) 전체를 마비시키고, 그 위에 함께 살고 있던 수십 개의 다른 파드들까지 동반 추락시키는 재앙을 초래할 수 있다.

플랫폼의 안정성은 개별 애플리케이션의 안정성만큼이나, 이 공유 자원을 어떻게 공정하고 예측 가능하게 배분하는가에 달려있다. 쿠버네티스는 이 문제를 해결하기 위해 모든 파드에게 입주 시 두 가지 약속을 하도록 요구한다. 바로 \*\*자원 요청(Requests)\*\*과 \*\*자원 제한(Limits)\*\*이다. 이 약속은 선택이 아닌, 성숙한 클러스터 운영을 위한 최소한의 의무다.

-----

### Requests: 스케줄러를 위한 예약된 좌석

\*\*자원 요청(Requests)\*\*은 당신의 파드가 \*\*'정상적으로 동작하기 위해 최소한으로 보장받아야 하는 자원의 양'\*\*을 의미한다. 이것은 파드가 클러스터에 처음 배치될 때, 즉 **스케줄링(Scheduling)** 단계에서 가장 결정적인 역할을 한다.

당신이 "내 파드는 CPU 1코어와 메모리 2GiB를 요청합니다(`requests`)"라고 선언했다고 생각해보자. 쿠버네티스 스케줄러는 이 요청서를 들고 클러스터의 모든 노드를 둘러본다. 그리고 이 요청을 만족시킬 만큼의 \*\*'사용 가능한 용량(allocatable resources)'\*\*이 남아있는 노드만을 최종 후보지로 간주한다. 만약 어떤 노드도 이 최소 요구사항을 만족시키지 못한다면, 당신의 파드는 영원히 `Pending` 상태에 머무르며 스케줄링되지 않을 것이다.

`requests`는 일종의 '예약석'과 같다. 일단 파드가 특정 노드에 자리를 잡고 나면, 쿠버네티스는 그 파드를 위해 요청된 만큼의 자원을 예약해둔다. 실제 그 파드가 그만큼의 자원을 사용하든 안 하든 상관없이, 다른 파드가 그 예약된 공간을 침범할 수 없다. 이것은 당신의 애플리케이션이 최소한의 생존 기반을 보장받는다는 강력한 약속이다.

### Limits: 노드의 안정을 위한 최후의 방어선

\*\*자원 제한(Limits)\*\*은 당신의 파드가 \*\*'어떤 상황에서도 절대로 초과해서 사용할 수 없는 자원의 상한선'\*\*을 규정한다. 이것은 파드가 이미 노드 위에서 실행 중일 때, **Kubelet**에 의해 강제되는 강력한 규칙이다.

만약 당신이 "내 파드는 CPU를 최대 2코어까지만, 메모리는 최대 4GiB까지만 사용하도록 제한합니다(`limits`)"라고 선언했다면, 다음과 같은 일이 벌어진다.

  * **CPU:** 당신의 파드가 2코어를 초과하는 연산을 시도하면, 커널은 그 파드의 CPU 사용 시간을 강제로 조절(**쓰로틀링, throttling**)하여 상한선을 넘지 못하도록 막는다. 이는 애플리케이션의 성능 저하를 유발할 수는 있지만, 적어도 노드 전체를 마비시키는 일은 막아준다.
  * **메모리:** 메모리는 CPU와 다르다. '조절'하는 것이 불가능하다. 만약 당신의 파드가 4GiB의 메모리 제한을 넘어서는 할당을 시도하면, 리눅스 커널의 **OOM(Out of Memory) Killer**가 즉시 출동하여 그 파드의 프로세스를 무자비하게 \*\*종료(kill)\*\*시켜 버린다. 이것이 바로 당신이 `kubectl describe pod`에서 자주 보게 될 `OOMKilled` 상태의 정체다. 고통스럽지만, 이 희생 덕분에 노드 위의 다른 파드들은 살아남을 수 있다.

### QoS Class: 파드의 서열을 결정하는 등급

쿠버네티스는 당신이 `requests`와 `limits`를 어떻게 설정했느냐에 따라 파드를 세 가지 **서비스 품질(Quality of Service, QoS)** 등급으로 자동 분류한다. 이 등급은 노드에 메모리 부족과 같은 극심한 자원 압박이 닥쳤을 때, **누가 먼저 퇴출될 것인가**를 결정하는 생존 서열과도 같다.

1.  **Guaranteed (최우선):** 파드의 모든 컨테이너에 대해 CPU와 Memory의 `requests`와 `limits` 값이 **정확히 동일하게** 설정된 경우. 이 파드는 약속한 자원을 온전히 보장받으며, 시스템이 죽기 직전까지는 절대로 퇴출되지 않는 VIP 승객이다.
2.  **Burstable (중간):** `requests`와 `limits`가 설정되어 있지만, 그 값이 서로 다른 경우. 이 파드는 평소에는 `requests`만큼의 자원을 보장받다가, 노드에 여유가 있을 때는 `limits`까지 자원을 '폭발적(burst)'으로 더 사용할 수 있다. 어느 정도의 안정성은 보장받지만, Guaranteed 파드를 살리기 위해 퇴출될 수 있다.
3.  **BestEffort (최하위):** `requests`와 `limits`가 **전혀 설정되지 않은** 경우. 이 파드는 노드에 남는 자원이 있을 때만 실행되는, 말 그대로 '최선 노력형' 승객이다. 자원 압박 상황에서 가장 먼저, 그리고 아무런 동정 없이 퇴출되는 1순위 대상이다.

다음은 이 모든 것을 종합한 YAML 명세다.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-app-pod
spec:
  containers:
  - name: my-app
    image: my-app:1.0.0
    resources:
      requests: # 최소 보장 요구량 (스케줄링 기준)
        memory: "2Gi"
        cpu: "1" # 1 core
      limits: # 절대 초과 불가 상한선 (런타임 강제)
        memory: "4Gi"
        cpu: "2"
```

이 파드는 `requests`와 `limits`가 다르므로 **Burstable** QoS 클래스로 분류된다.

`requests`와 `limits`를 설정하는 것은 단순히 YAML 파일에 몇 줄을 추가하는 행위가 아니다. 그것은 당신의 애플리케이션이 클러스터라는 생태계의 책임감 있는 일원으로서, 이웃과 공존하기 위한 최소한의 사회적 계약에 서명하는 것과 같다. 이 계약을 무시하는 것은 결국 당신의 플랫폼 전체를 예측 불가능한 혼돈으로 몰아넣는 지름길이다.

이제 우리는 애플리케이션을 제대로 설계하고, 컨테이너에 담아, 클러스터 위에서 안정적으로 실행하고 이웃과 공존하는 법까지 배웠다. 하지만 애플리케이션의 수가 늘어나고 환경(개발, 스테이징, 운영)이 복잡해지면서, 우리는 이 모든 것을 기술하는 수많은 YAML 파일의 홍수, 즉 'YAML 지옥'이라는 새로운 도전에 직면하게 된다. 다음 장에서는 이 지옥에서 벗어나기 위한 선언적 구성 관리의 기술들을 마스터해볼 것이다.