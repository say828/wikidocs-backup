# 13장: 서비스 메시 시대: 애플리케이션 네트워킹의 진화

우리는 이제 쿠버네티스 API를 우리 마음대로 확장하여, 클러스터 안팎의 모든 것을 선언적으로 관리할 수 있는 궁극의 플랫폼 제어 능력을 손에 넣었다. 우리의 플랫폼은 견고하고, 자동화되어 있으며, 확장 가능하다. 하지만 애플리케이션의 수가 수십, 수백 개로 늘어나고, 이들이 서로 거미줄처럼 얽혀 통신하기 시작하면서, 우리는 쿠버네티스의 기본 네트워킹만으로는 해결할 수 없는 새로운 차원의 혼돈과 마주하게 된다. 바로 **애플리케이션 네트워킹(Application Networking)**의 복잡성이다. 🕸️

어떤 서비스가 다른 서비스와 통신할 때, 우리는 다음과 같은 수많은 질문에 답해야 한다.
* **신뢰와 보안:** A 서비스가 B 서비스와 통신할 때, A는 B가 정말 B가 맞는지 어떻게 신뢰하며, 그 둘 사이의 통신은 어떻게 암호화할 것인가?
* **회복탄력성:** B 서비스가 일시적으로 응답이 없다면, A는 몇 번이나 재시도(retry)해야 하는가? B 서비스에 장애가 전파되는 것을 막기 위해 언제 연결을 끊어야(circuit breaking) 하는가?
* **관측 가능성:** A와 B 사이에 오고 가는 모든 요청의 성공률, 지연 시간은 어떻게 측정하며, 어떤 요청이 실패했는지 어떻게 추적할 것인가?

과거에는 이 모든 로직—재시도, 타임아웃, 서킷 브레이커, mTLS, 메트릭 수집—을 개발자가 직접 애플리케이션 코드 안에, 각 언어별로, 각기 다른 라이브러리(Netflix Hystrix, Resilience4j 등)를 사용하여 구현해야 했다. 이는 엄청난 중복 작업일 뿐만 아니라, 비즈니스 로직과 인프라 로직이 뒤섞여 코드를 복잡하게 만들고, 조직 전체의 기술 스택 파편화를 초래했다.

이 장에서는 바로 이 혼돈을 애플리케이션 코드로부터 완전히 분리하여, 플랫폼 계층에서 투명하게 처리하는 혁신적인 패러다임, **서비스 메시(Service Mesh)**의 세계로 들어간다. 우리는 서비스 메시가 정확히 어떤 문제를 해결하며 왜 필요한지를 먼저 이해하고, 오늘날 시장을 양분하고 있는 두 거인, **Istio**와 **Linkerd**의 아키텍처를 전격 비교 분석할 것이다. 나아가, **mTLS**를 통해 어떻게 모든 서비스 간 통신을 제로 트러스트 원칙에 따라 자동으로 암호화하는지 배우고, 마지막으로 서비스 메시를 활용하여 블루/그린, 카나리를 넘어선 정교한 트래픽 관리와 의도적으로 장애를 주입하여 시스템의 약점을 찾는 **장애 주입(Fault Injection)** 테스트의 세계를 경험하게 될 것이다.

---

## 00. 서비스 메시는 어떤 문제를 해결하는가: 필요성과 핵심 기능

서비스 메시의 핵심 아이디어는 2장에서 우리가 만났던 **사이드카 패턴(Sidecar Pattern)**을 극단까지 밀어붙인 것이다. 서비스 메시는 클러스터의 모든 애플리케이션 파드에, 애플리케이션 컨테이너와 나란히 동작하는 매우 지능적이고 강력한 **네트워크 프록시(Network Proxy)** 사이드카(보통 Envoy나 linkerd-proxy)를 **자동으로 주입**한다.

이제부터 애플리케이션 컨테이너는 더 이상 다른 서비스를 직접 호출하지 않는다. `frontend`가 `backend`를 호출하면, 그 요청은 같은 파드 안에 있는 사이드카 프록시에 의해 가로채진다. 이 프록시는 `backend` 파드의 사이드카 프록시와 통신하고, `backend`의 사이드카 프록시가 다시 같은 파드 안의 `backend` 애플리케이션 컨테이너에게 요청을 전달한다. 애플리케이션은 이 모든 과정이 `localhost` 통신처럼 보일 뿐, 자신의 곁에 그림자처럼 붙어있는 이 프록시의 존재를 전혀 인지하지 못한다.



이렇게 모든 서비스 간 통신이 이 프록시들의 '메시(mesh, 그물망)'를 통과하게 되면서, 우리는 다음과 같은 마법 같은 능력들을 얻게 된다.

* **관측 가능성 (Observability):** 모든 트래픽이 프록시를 지나가므로, 우리는 코드 한 줄 건드리지 않고도 모든 서비스 간의 초당 요청 수(RPS), 에러율, 응답 시간 분포(P95, P99)와 같은 황금 신호(Golden Signals) 메트릭을 자동으로 얻을 수 있다.
* **보안 (Security):** 프록시들은 서로 통신을 시작하기 전에 상대방의 신원을 암호학적으로 확인하고, 모든 트래픽을 상호 TLS(mTLS)로 자동 암호화할 수 있다. "A 서비스는 오직 B 서비스하고만 통신할 수 있다"와 같은 강력한 인증/인가 정책을 코드 변경 없이 적용할 수 있다.
* **신뢰성 (Reliability):** `backend` 서비스가 일시적으로 503 에러를 반환하면, `frontend`의 사이드카 프록시가 자동으로 요청을 재시도(retry)해 줄 수 있다. `backend`의 장애율이 특정 임계치를 넘으면, 프록시는 더 이상의 요청이 `backend`로 가는 것을 막는 서킷 브레이커(circuit breaker) 역할을 수행하여 장애의 확산을 막는다.
* **트래픽 제어 (Traffic Control):** 우리는 이 프록시들에게 "들어오는 요청의 90%는 v1 서비스로, 10%는 v2 서비스로 보내라" (카나리 배포), "HTTP 헤더에 `user-agent: mobile`이 포함된 요청은 모바일 전용 v3 서비스로 보내라" (콘텐츠 기반 라우팅)와 같은 매우 정교한 L7 라우팅 규칙을 동적으로 지시할 수 있다.

이 모든 복잡한 네트워크 로직이 애플리케이션 코드로부터 플랫폼 인프라 계층으로 완전히 **분리**되고 **추상화**된 것이다. 개발자는 이제 순수한 비즈니스 로직에만 집중할 수 있게 되고, 플랫폼 팀은 조직 전체의 네트워크 정책과 신뢰성 패턴을 중앙에서 일관되게 관리하고 적용할 수 있게 된다.

서비스 메시는 바로 이 '사이드카 프록시들의 집합'인 **데이터 플레인(Data Plane)**과, 이 모든 프록시들의 동작을 중앙에서 지휘하고 설정하는 '두뇌' 역할의 **컨트롤 플레인(Control Plane)**으로 구성된다.

이제 우리는 이 컨트롤 플레인을 구현하는 두 가지 서로 다른 철학, Istio와 Linkerd의 세계로 들어가 그들의 아키텍처를 비교해 볼 시간이다.