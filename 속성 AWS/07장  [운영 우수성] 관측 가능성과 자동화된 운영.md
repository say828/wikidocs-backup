# Part 3: 실전 아키텍처 설계: Well-Architected Framework 6대 원칙 적용

---

# 07장: [운영 우수성] 관측 가능성과 자동화된 운영

우리는 Part 2의 긴 여정을 통해 클라우드 네이티브 아키텍처라는 정교한 건축물의 뼈대와 장기를 모두 완성했다. 코드로 정의된 인프라(IaC) 위에 컴퓨팅, 데이터, 네트워크, 그리고 메시징이라는 핵심 구성 요소들이 제자리를 잡았다. 하지만 아무리 완벽하게 설계된 건축물이라도, 그 안의 전기, 수도, 환기 시스템을 효율적으로 운영하고 문제가 생겼을 때 신속하게 원인을 찾아 해결할 수 없다면, 그것은 그저 화려하지만 살 수 없는 집에 불과하다.

이제 우리는 건축가의 역할을 넘어, 이 복잡한 시스템을 24시간 365일 안정적으로 운영하고 지속적으로 개선해 나가는 **운영 총괄 책임자**의 관점으로 전환해야 한다. 이것이 바로 AWS Well-Architected Framework의 첫 번째 원칙, **운영 우수성(Operational Excellence)**의 본질이다. 운영 우수성은 단순히 '장애가 나지 않게 하는 것'을 넘어, 시스템의 상태를 깊이 있게 이해하고, 변경 사항을 안전하고 신속하게 배포하며, 예상치 못한 문제에 효과적으로 대응하는 모든 프로세스와 문화를 포괄한다.

이 장에서는 운영 우수성의 초석이 되는 **관측 가능성(Observability)**의 세 기둥—로깅, 메트릭, 트레이스—을 세우는 것부터 시작한다. 우리는 Amazon CloudWatch와 AWS X-Ray를 활용하여 분산된 마이크로서비스의 건강 상태를 손금 보듯 들여다보는 방법을 배우고, 코드 변경에서 프로덕션 배포까지의 전 과정을 자동화하는 CI/CD 파이프라인을 구축할 것이다. 마지막으로, 우리는 장애를 두려워하며 피하는 대신, 의도적으로 장애를 주입하여 시스템의 약점을 찾아내는 **카오스 엔지니어링(Chaos Engineering)**이라는 담대한 실천을 통해 진정한 회복탄력성을 검증하게 될 것이다.

---

## 00. 로깅, 메트릭, 트레이스: 시스템의 건강 상태를 파악하는 세 가지 방법

과거 모놀리식 애플리케이션을 운영하던 시절, 시스템에 문제가 생기면 우리의 접근법은 비교적 단순했다. 해당 서버에 접속해서 CPU와 메모리 사용률(**메트릭**)을 확인하고, 문제가 될 만한 시점의 애플리케이션 로그 파일(**로깅**)을 열어 에러 메시지를 찾는 것이 일반적인 문제 해결 방식이었다. 이는 마치 의사가 환자의 체온을 재고(메트릭), 언제부터 아팠는지 물어보는(로깅) 것과 같았다.

그러나 수십, 수백 개의 마이크로서비스가 서로 복잡한 비동기 통신을 주고받는 분산 환경에서는 이러한 전통적인 **모니터링(Monitoring)** 방식만으로는 충분하지 않다. 주문 서비스의 API 응답이 느려졌을 때, 그 원인이 주문 서비스 자체의 문제인지, 아니면 그 서비스가 호출하는 재고 서비스의 문제인지, 혹은 그 둘 사이의 네트워크 문제인지, 아니면 전혀 예상치 못한 제3의 서비스가 보낸 이벤트 때문인지 알아내는 것은 거의 불가능에 가깝다. 우리는 이제 단순히 시스템의 겉으로 드러난 증상을 관찰하는 것을 넘어, 시스템 내부의 상태를 외부로 질의하고 이해할 수 있는 능력, 즉 **관측 가능성(Observability)**을 확보해야 한다.

관측 가능성은 시스템이 얼마나 잘 작동하는지를 그 시스템이 외부에 드러내는 데이터를 통해 추론할 수 있는 능력을 의미한다. 그리고 이 관측 가능성을 떠받치는 세 개의 핵심 기둥이 바로 **메트릭(Metrics), 로깅(Logging), 그리고 트레이싱(Tracing)**이다. 이 셋은 서로를 대체하는 관계가 아니라, 각기 다른 질문에 답하며 서로를 보완하는 상호 보완적인 관계다.



1.  **메트릭 (Metrics) - "무엇이 문제인가?" (What?)**
    * **본질:** 시간의 흐름에 따라 수집된, 시스템의 특정 속성을 나타내는 **숫자 데이터**다. 예를 들어, CPU 사용률, 네트워크 입출력, API 호출 횟수, 에러율, 응답 시간(Latency) 등이 모두 메트릭이다.
    * **역할:** 시스템의 전반적인 건강 상태와 트렌드를 알려주는 **계기판** 역할을 한다. 우리는 메트릭을 통해 "API 응답 시간이 평소보다 2배 느려졌다"거나 "오류율이 5%를 넘어섰다"는 **'문제의 발생'**을 가장 먼저 인지할 수 있다. 메트릭은 경보(Alarm)를 울리는 훌륭한 조기 경보 시스템이다. 하지만 왜 응답 시간이 느려졌는지, 어떤 요청에서 오류가 발생했는지에 대한 '이유'는 알려주지 못한다.

2.  **로깅 (Logging) - "왜 문제가 발생했는가?" (Why?)**
    * **본질:** 시스템에서 발생한 **개별적이고 불연속적인 사건(Event)에 대한 타임스탬프가 찍힌 기록**이다. 애플리케이션이 특정 코드 라인을 실행했거나, 오류가 발생했거나, 특정 요청을 받았을 때의 상세한 컨텍스트가 텍스트 형태로 기록된다.
    * **역할:** 메트릭이 알려준 문제의 근본적인 **'원인'을 파악**하기 위한 단서를 제공하는 **사건 일지**다. API 응답 시간이 느려졌다는 경보를 받았다면, 우리는 해당 시점의 로그를 분석하여 "특정 사용자의 비정상적인 요청으로 인해 데이터베이스 커넥션 풀이 고갈되었다"는 식의 구체적인 이유를 찾아낼 수 있다.

3.  **트레이싱 (Tracing) - "어디서 문제가 발생했는가?" (Where?)**
    * **본질:** 분산 시스템 환경에서, 단일 요청이 여러 마이크로서비스를 거쳐 처리되는 **전체 여정을 시각적으로 추적**하는 것이다. 각 서비스에서의 작업 시간을 스팬(Span)이라는 단위로 측정하고, 이들을 하나의 고유한 트레이스 ID(Trace ID)로 묶어 요청의 전체 흐름과 병목 구간을 보여준다.
    * **역할:** 문제의 **'위치'와 '범위'를 특정**하는 **GPS 추적 시스템**이다. 주문 API의 응답 시간이 3초가 걸렸다는 사실을 알게 되었을 때, 트레이스를 살펴보면 "주문 서비스에서 0.2초, 재고 서비스에서 2.5초, 결제 서비스에서 0.3초가 소요되었다"는 사실을 명확히 알 수 있다. 이를 통해 우리는 문제 해결의 노력을 엉뚱한 곳이 아닌, 진짜 병목 지점인 재고 서비스에 집중할 수 있다.

결론적으로, 이 세 가지는 분리될 수 없는 삼위일체와 같다. **메트릭**으로 문제를 감지하고, **트레이스**로 문제의 위치를 찾으며, **로그**로 문제의 근본 원인을 파헤친다. 진정한 운영 우수성은 이 세 가지 데이터를 유기적으로 연결하여, 미지의 장애(Unknown Unknowns)까지도 신속하게 진단하고 해결할 수 있는 능력을 갖추는 것에서 시작된다. 이제, AWS의 핵심 관측 가능성 서비스인 CloudWatch와 X-Ray를 통해 이 세 기둥을 어떻게 실제로 구축하는지 살펴보자.