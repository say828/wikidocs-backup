# 08장: 대용량 이벤트를 위한 Apache Kafka 심층 분석

## 00. 왜 Kafka인가?: 대규모 트래픽과 실시간 스트리밍의 요구

07장에서 우리는 `Spring Cloud Stream`이라는 '추상화' 계층 뒤에서 Kafka를 편리하게 사용했습니다. 개발자는 Kafka의 내부 동작을 몰라도 이벤트를 발행하고 구독할 수 있었습니다.

하지만 진정한 아키텍트는 자신이 사용하는 기술의 '내부'를 이해해야 합니다. 왜 세상의 수많은 빅테크 기업(Uber, Netflix, LinkedIn...)들이 메시지 브로커의 표준으로 Kafka를 채택했을까요? 단순히 RabbitMQ보다 '빨라서'일까요?

그 답은 현대 애플리케이션이 요구하는 두 가지 핵심 키워드, **'대규모 트래픽'**과 **'실시간 스트리밍'**에 있습니다. Kafka는 단순한 메시지 큐(Message Queue)가 아니라, 이 두 가지를 모두 만족시키기 위해 탄생한 **분산 스트리밍 플랫폼(Distributed Streaming Platform)**이기 때문입니다.

---

### 1. 대규모 트래픽 (Massive Throughput)의 요구

우리가 지금까지 다룬 `OrderPaidEvent`는 사실 빙산의 일각에 불과합니다. 성공적인 이커머스 플랫폼은 초당 수만, 수백만 건의 다양한 이벤트를 생성합니다.

* 모든 사용자의 **클릭 이벤트** (`ClickEvent`)
* 모든 상품 **조회 이벤트** (`ProductViewEvent`)
* 모든 **장바구니 담기** 이벤트 (`AddToCartEvent`)
* 백엔드 애플리케이션의 모든 **로그(Log)** 데이터
* 서버의 **메트릭(Metric)** 데이터

이처럼 엄청난 양의 데이터를 전통적인 RDBMS나 메시지 큐(RabbitMQ)에 안정적으로, 그리고 빠르게 기록하는 것은 거의 불가능에 가깝습니다.

**Kafka는 어떻게 이 문제를 해결했는가?**
Kafka는 '메시지'를 데이터베이스처럼 복잡하게 관리하는 대신, **'커밋 로그(Commit Log)'**라는 단순하지만 극도로 효율적인 자료 구조를 채택했습니다.

1.  **Append-Only (추가만 가능):** 모든 이벤트는 로그 파일의 '끝'에 순차적으로 추가만 됩니다. DB처럼 특정 데이터를 찾아 수정(Update)하거나 삭제(Delete)하는 복잡한 연산이 없습니다.
2.  **순차 I/O (Sequential I/O):** 디스크에 데이터를 순차적으로 쓰는 작업은, 메모리에서 임의의 데이터를 찾는 것보다 빠를 수 있습니다. Kafka는 이 하드웨어의 물리적 특성을 극대화하여 압도적인 쓰기 성능을 확보합니다.
3.  **Zero-Copy:** 리눅스 OS의 `sendfile` 시스템 콜을 사용하여, 디스크에 있는 데이터를 애플리케이션(JVM) 메모리로 복사하는 과정 없이, 커널 레벨에서 바로 네트워크로 전송합니다. 이는 데이터 전송 오버헤드를 극적으로 줄여줍니다.

이러한 Low-Level 최적화를 통해, Kafka는 일반적인 하드웨어에서도 초당 수십만 건 이상의 이벤트를 처리하는 괴물 같은 성능을 보여줍니다.

---

### 2. 실시간 스트리밍 (Real-time Streaming)의 요구

과거의 시스템은 데이터를 모아 DB에 저장한 뒤, 밤에 '배치(Batch)' 작업을 돌려 분석했습니다. 하지만 현대 비즈니스는 **'데이터가 발생하는 순간'**에 즉시 분석하고 반응하기를 원합니다.

* **실시간 사기 탐지:** 결제 이벤트 스트림을 실시간으로 분석하여, 이상 패턴이 감지되면 결제가 완료되기 전에 즉시 차단해야 합니다.
* **실시간 개인화 추천:** 사용자의 클릭 스트림을 기반으로, 바로 다음 페이지에서 사용자가 좋아할 만한 상품을 추천해야 합니다.
* **실시간 재고 관리:** 전 세계 매장의 POS기에서 발생하는 판매 이벤트 스트림을 중앙에서 실시간으로 집계하여 재고를 관리해야 합니다.

**Kafka는 어떻게 이 문제를 해결했는가?**
Kafka의 핵심은 "메시지를 소비한 뒤 바로 삭제하는" 전통적인 큐가 아니라, **"이벤트를 정해진 기간(예: 7일) 동안 영구적으로 보관하는"** 로그라는 점입니다.



이 '데이터 보관' 특성은 놀라운 유연성을 제공합니다.
1.  **다수의 독립적인 소비자:** `OrderPaidEvent`라는 **동일한 이벤트 스트림**을 가지고,
    * `shipping-service`는 '배송'을 위해 한 번 소비하고,
    * `analytics-service`는 '실시간 매출 분석'을 위해 또 한 번 소비하고,
    * `fraud-detection-service`는 '사기 탐지'를 위해 또 한 번 소비할 수 있습니다.
    각 소비자는 다른 소비자에게 전혀 영향을 주지 않고, 각자의 '책갈피(오프셋)'를 기준으로 독립적으로 이벤트를 읽어갑니다.

2.  **데이터 재생 (Replay):** 1년 뒤, 새로운 '머신러닝 추천 모델'을 구축하기 위해 과거 7일간의 모든 주문 데이터가 필요해졌다고 가정해 봅시다. Kafka를 사용하면, `order-service`를 전혀 건드리지 않고, 새로운 컨슈머를 만들어 **Topic의 처음부터 모든 이벤트를 다시 읽어와(Replay)** 모델을 학습시킬 수 있습니다.

---

결론적으로, Kafka는 단순히 메시지를 안정적으로 전달하는 큐를 넘어, 대규모 데이터를 실시간으로 수집, 저장, 처리할 수 있는 **'데이터의 중추 신경계'** 역할을 수행합니다.

이 장에서는 Kafka의 이러한 강력함을 가능하게 하는 핵심 아키텍처 구성요소인 **토픽(Topic), 파티션(Partition), 그리고 오프셋(Offset)**의 원리를 깊이 있게 파헤쳐 볼 것입니다.