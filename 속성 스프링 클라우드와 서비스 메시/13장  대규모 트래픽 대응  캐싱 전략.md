# 13장: 대규모 트래픽 대응: 캐싱 전략

## 00. 왜 캐시가 필요한가?: 응답 속도 향상과 DB 부하 감소

12장에서 우리는 `WebFlux`와 `코루틴`을 도입하여, 적은 스레드로 많은 요청을 처리할 수 있는 고성능 논블로킹 애플리케이션의 기반을 마련했습니다. 하지만 논블로킹 I/O라 할지라도, 데이터베이스를 조회하는 작업은 여전히 네트워크 통신과 디스크 접근을 수반하며, 이는 **메모리 접근 속도보다 수천, 수만 배 느립니다.**

우리 이커머스 사이트에서 가장 인기 있는 '특가 상품'의 상세 페이지를 생각해 봅시다. 이 페이지는 1초에 1,000번 이상 조회될 수 있습니다. 상품의 이름, 설명, 가격과 같은 데이터는 거의 변하지 않는데도, 이 1,000번의 요청이 매번 `product-service`의 데이터베이스(RDBMS 또는 Elasticsearch)를 조회해야 할까요? 이는 엄청난 자원 낭비이며, 시스템의 가장 큰 병목 지점이 됩니다.

이 문제를 해결하는 가장 효과적이고 보편적인 방법이 바로 **캐싱(Caching)**입니다.

---

### 캐시(Cache)란 무엇인가?

**캐시**는 자주 접근하지만 잘 변하지 않는 데이터를, 원본 저장소(데이터베이스)보다 훨씬 더 빠른 저장소(주로 **인메모리**)에 **일시적으로 복사해두는** 기술입니다.

**도서관과 책상 비유 📚:**
* **데이터베이스:** 중앙 **도서관**입니다. 책(데이터)이 필요할 때마다 도서관까지 걸어가서, 책을 찾아, 자리로 돌아와야 합니다. 시간이 걸립니다.
* **캐시:** 내 **책상**입니다. 오늘 하루 종일 볼 책(자주 쓰는 데이터)이 있다면, 아침에 도서관에서 한 번만 빌려와 책상 위에 펼쳐둡니다. 그 후 책이 필요할 때마다, 손만 뻗으면 즉시 볼 수 있습니다.



---

캐싱은 두 가지 핵심적인 이점을 제공하여 시스템의 운명을 바꿉니다.

### 1. 폭발적인 응답 속도 향상 (Latency 감소)

사용자 경험에 가장 직접적인 영향을 미칩니다.

* **메모리 접근:** `Redis`와 같은 인메모리 캐시에서 데이터를 가져오는 데 걸리는 시간은 보통 **1밀리초(ms) 미만**입니다. (마이크로초 단위)
* **데이터베이스 접근:** 원격 데이터베이스에서 데이터를 가져오는 데는 네트워크 왕복 시간과 디스크 I/O 시간이 더해져 **수십 ~ 수백 밀리초(ms)**가 걸립니다.

캐시를 사용하면, 사용자는 상품 상세 페이지가 눈 깜짝할 사이에 로딩되는 것을 경험하게 됩니다. 이는 고객 만족도와 직접적으로 연결됩니다.

### 2. 데이터베이스 부하 극적 감소 (DB 보호)

이는 시스템 전체의 안정성과 확장성에 결정적인 영향을 미칩니다.

압도적으로 많은 '읽기(Read)' 요청을 캐시가 대신 처리해주면, 데이터베이스는 과부하로부터 보호받습니다. 이는 마치 DB 앞에 거대한 '방패'를 세우는 것과 같습니다.

이 '방패' 덕분에 데이터베이스는 자신의 소중한 자원(DB 커넥션, CPU, I/O)을 정말로 중요한 작업, 즉 **데이터의 일관성을 보장해야 하는 '쓰기(Write)' 작업**(예: 주문 생성 시 재고 차감)에 집중할 수 있게 됩니다. 읽기 트래픽이 아무리 폭증해도, 쓰기 트랙잭션은 안정적으로 처리될 수 있습니다.

---

**결론적으로,** 캐싱은 선택적인 '최적화' 기법이 아니라, 대규모 트래픽을 다루는 현대적인 웹 애플리케이션을 위한 **필수 아키텍처 패턴**입니다.

물론 캐시를 도입하는 것은 공짜가 아닙니다. "원본 데이터(DB)가 변경되었을 때, 캐시의 복사본을 어떻게 최신 상태로 유지할 것인가?"라는 **캐시 일관성(Cache Coherency)**이라는 어려운 숙제가 따라옵니다.

이 장에서는 캐시의 종류(로컬 캐시 vs 분산 캐시)를 알아보고, Spring Cache 추상화를 통해 우리 이커머스 애플리케이션에 캐시를 적용하며, 이 '캐시 일관성' 문제를 해결하기 위한 다양한 전략을 탐구할 것입니다.