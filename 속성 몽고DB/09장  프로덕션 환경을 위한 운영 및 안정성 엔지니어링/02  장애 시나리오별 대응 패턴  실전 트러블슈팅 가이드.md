## 02\. 장애 시나리오별 대응 패턴: 실전 트러블슈팅 가이드

견고한 아키텍처와 꼼꼼한 모니터링에도 불구하고, 프로덕션 환경에서는 예상치 못한 장애가 발생하기 마련입니다. 장애의 영향을 최소화하고 신속하게 서비스를 정상화하는 능력은 시스템의 신뢰도를 결정짓는 핵심 역량입니다.

이번 절에서는 실제 운영 환경에서 마주할 수 있는 대표적인 장애 시나리오들을 살펴보고, 문제의 원인을 진단하고 해결하기 위한 체계적인 트러블슈팅 패턴을 제시합니다. 이것은 SRE와 데브옵스 엔지니어를 위한 응급 상황 대응 매뉴얼입니다.

-----

### 시나리오 1: 쿼리 지연 시간(Latency) 급증

  * **증상:** "평균 쿼리 지연 시간 500ms 초과" 알람 발생. 사용자들이 애플리케이션이 느리다고 불평하기 시작합니다.

#### 진단 절차

1.  **범위 특정:** 전체 시스템이 느린가, 아니면 특정 쿼리만 느린가?

      * **`db.currentOp()`** 명령어나 Atlas Performance Advisor를 사용하여 현재 실행 중인 쿼리 중 유독 오래 걸리는(`secs_running`이 높은) 작업을 식별합니다.

    <!-- end list -->

    ```javascript
    db.currentOp({ "active": true, "secs_running": { "$gt": 5 } })
    ```

2.  **원인 분석 (느린 쿼리 식별 시):**

      * 해당 쿼리에 대해 즉시 **`.explain("executionStats")`** 를 실행합니다.
      * **`COLLSCAN`** 이 있는가? 십중팔구 이것이 범인입니다. 인덱스 없이 대용량 컬렉션을 전체 스캔하고 있을 가능성이 높습니다.
      * **`IXSCAN`** 이지만 `totalDocsExamined` 수가 `nReturned` 수에 비해 비정상적으로 높은가? 이는 인덱스는 사용했지만, 쿼리를 충분히 효율적으로 좁혀주지 못하는, 선택성(selectivity)이 낮은 인덱스를 사용하고 있다는 의미입니다.

3.  **원인 분석 (시스템 전반이 느릴 시):**

      * **리소스 포화 상태 확인:** 모니터링 대시보드를 통해 핵심 시스템 리소스를 확인합니다.
          * **CPU 사용률 \> 90%:** `COLLSCAN`, 인덱스 없는 정렬(`SORT` 스테이지), 복잡한 애그리게이션 등이 원인일 수 있습니다.
          * **디스크 I/O 대기(iowait) 급증:** 워킹셋이 RAM보다 커서 지속적인 페이지 폴트와 디스크 읽기가 발생하고 있을 가능성이 높습니다.
          * **읽기/쓰기 대기열(`queued`) 증가:** 쿼리들이 락(lock)을 얻기 위해 대기하고 있습니다. 장시간 실행되는 쓰기 작업이나 인덱스 빌드 등이 다른 작업들을 막고 있을 수 있습니다.

#### 대응 패턴

  * **단기 (응급 조치):**
      * 특정 악성 쿼리가 시스템 전체에 영향을 주고 있다면, `db.killOp()`를 사용하여 해당 쿼리를 즉시 강제 종료하고 서비스를 정상화합니다.
  * **장기 (근본 해결):**
      * `COLLSCAN`이 원인이라면, 쿼리 조건에 맞는 **최적의 인덱스를 생성**합니다.
      * 워킹셋이 RAM보다 큰 것이 원인이라면, **서버의 RAM을 증설**하는 것이 가장 확실한 해결책입니다.

-----

### 시나리오 2: 복제 지연(Replication Lag) 시간 초과

  * **증상:** "세컨더리 노드의 복제 지연이 60초를 초과" 알람 발생.

#### 진단 절차

1.  **프라이머리 확인:** 프라이머리에 비정상적인 대규모 쓰기 작업(대용량 데이터 마이그레이션 등)이 발생하여 Oplog 생성 속도가 너무 빨라졌는지 확인합니다.
2.  **네트워크 확인:** 프라이머리와 해당 세컨더리 간의 네트워크 지연 시간이나 패킷 손실이 없는지 `ping`, `traceroute` 등으로 확인합니다.
3.  **세컨더리 확인 (가장 흔한 원인):**
      * **세컨더리에서 무거운 읽기 작업이 실행 중인가?** BI 도구나 데이터 분석가가 해당 세컨더리 노드를 대상으로 리소스를 많이 소모하는 애그리게이션 쿼리를 실행하고 있을 수 있습니다. `db.currentOp()`를 **세컨더리 노드에서 직접 실행**하여 확인합니다.
      * **리소스 부족:** 해당 세컨더리 노드의 CPU, 디스크 I/O가 포화 상태인가? 프라이머리보다 낮은 사양의 하드웨어를 사용하는 세컨더리에서 병목 현상이 발생하는 경우가 많습니다.

#### 대응 패턴

  * **단기 (응급 조치):**
      * 세컨더리에서 실행 중인 무거운 분석 쿼리가 원인이라면 `db.killOp()`로 중단시켜 복제를 정상화합니다.
  * **장기 (근본 해결):**
      * 분석 쿼리 전용으로 사용할 **히든(hidden) 멤버** 또는 분석 전용 클러스터를 구성하여, 읽기 부하와 복제 부하를 분리합니다.
      * 모든 레플리카 셋 멤버의 하드웨어 사양을 동일하게 유지합니다.

-----

### 시나리오 3: 연결(Connection) 수 급증

  * **증상:** "활성 연결 수가 최대치의 80% 초과" 알람 발생. 새로운 연결 요청이 실패할 수 있습니다.

#### 진단 절차

1.  **애플리케이션 배포 확인:** 최근에 새로운 버전의 애플리케이션을 배포했거나, 서버 수를 늘렸는지 확인합니다. 이 경우 전체 연결 수가 자연스럽게 증가할 수 있습니다.
2.  **커넥션 풀(Connection Pool) 설정 확인:** 애플리케이션 드라이버의 커넥션 풀 설정이 비효율적인 것은 아닌지 확인합니다. 너무 짧은 유휴 시간(idle time) 설정으로 인해 불필요한 연결/해제 작업이 반복될 수 있습니다.
3.  **느린 쿼리 확인:** 느린 쿼리가 커넥션 풀의 연결을 오랫동안 점유하고 반환하지 않으면, 다른 요청들이 새로운 연결을 계속 생성하여 전체 연결 수가 증가할 수 있습니다. (시나리오 1 참고)

#### 대응 패턴

  * 애플리케이션 서버 수에 맞춰 데이터베이스의 최대 연결 수(`maxIncomingConnections`)를 재산정하고 조정합니다.
  * 애플리케이션 드라이버의 커넥션 풀 설정을 최적화합니다.
  * 느린 쿼리를 해결하여 커넥션이 빠르게 풀에 반환되도록 합니다.

체계적인 트러블슈팅의 핵심은 **관찰(증상 파악) → 가설 수립(원인 추론) → 진단(데이터로 증명) → 조치**의 사이클을 침착하게 따르는 것입니다. 이 가이드는 그 출발점을 제공하는 지도와 같습니다.